<!DOCTYPE html>
<html>
  <head>
    <title>Applied Machine Learning - Basic Principals</title>
    <meta charset="utf-8">
    <meta name="author" content="Max Kuhn (RStudio)" />
    <link href="libs/remark-css-0.0.1/example.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
    <script src="libs/viz-0.3/viz.js"></script>
    <link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
    <script src="libs/grViz-binding-0.9.0/grViz.js"></script>
    <script src="libs/jquery-1.12.4/jquery.min.js"></script>
    <link href="libs/leaflet-0.7.7/leaflet.css" rel="stylesheet" />
    <script src="libs/leaflet-0.7.7/leaflet.js"></script>
    <link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
    <link href="libs/leaflet-label-0.2.2/leaflet.label.css" rel="stylesheet" />
    <script src="libs/leaflet-label-0.2.2/leaflet.label.js"></script>
    <script src="libs/Proj4Leaflet-0.7.2/proj4-compressed.js"></script>
    <script src="libs/Proj4Leaflet-0.7.2/proj4leaflet.js"></script>
    <script src="libs/leaflet-binding-1.1.0/leaflet.js"></script>
    <script src="libs/leaflet-providers-1.0.27/leaflet-providers.js"></script>
    <script src="libs/leaflet-providers-plugin-1.1.0/leaflet-providers-plugin.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Applied Machine Learning - Basic Principals
### Max Kuhn (RStudio)

---





# Introduction

In this section, we will introduce concepts that are useful for any type of machine learning model. 

Many of these topics will be put into action in later sections. 


---

# The Modeling _Process_

Common steps during model building are:

* estimating model parameters (i.e. training models)

* determining the values of _tuning parameters_ that cannot be directly calculated from the data

* model selection (within a model type) and model comparison (between types)

* calculating the performance of the final model that will generalize to new data

Many books and courses portray predictive modeling as a short sprint. A better analogy would be a marathon or campaign (depending on how hard the problem is). 


---

# What the Modeling Process Usually Looks Like

&lt;img src="intro-process-1.svg"&gt;


---

# Data Splitting and Spending

How do we "spend" the data to find an optimal model? 

We _typically_ split data into training and test data sets:

*  ***Training Set***: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.

*  ***Test Set***: these data can be used to get an independent assessment of model efficacy. They should not be used during model training. 


---

# Data Splitting and Spending

The more data we spend, the better estimates we'll get (provided the data is accurate).  

Given a fixed amount of data:

* too much spent in training won't allow us to get a good assessment of predictive performance.  We may find a model that fits the training data very well, but is not generalizable (overfitting)

* too much spent in testing won't allow us to get a good assessment of model parameters

Statistically, the best course of action would be to use all the data for model building and use statistical methods to get good estimates of error.

From a non-statistical perspective, many consumers of complex models emphasize the need for an untouched set of samples to evaluate performance.


---

# Large Data Sets

When a large amount of data are available, it might seem like a good idea to put a large amount into the training set. _Personally_, I think that this causes more trouble than it is worth due to diminishing returns on performance and the added cost and complexity of the required infrastructure. 

Alternatively, it is probably a better idea to reserve good percentages of the data for specific parts of the modeling process. For example: 

* Save a large chunk of data to perform feature selection prior to model building
* Retain data to calibrate class probabilities or determine a cutoff via an ROC curve. 

Also, there may be little need for iterative resampling of the data. A single holdout (aka validation set) may be sufficient in some cases if the data are large enough and the data sampling mechanism is solid.  


---

# Mechanics of Data Splitting

There are a few different ways to do the split: simple random sampling, _stratified sampling based on the outcome_, by date, or methods that focus on the distribution of the predictors.

For stratification:

* **classification**: this would mean sampling within the classes as to preserve the distribution of the outcome in the training and test sets

* **regression**: determine the quartiles of the data set and samples within those artificial groups


---

# Ames Housing Data

Let's load the example data set and split it. We'll put 75% into training and 25% into testing. 


```r
library(AmesHousing)
ames &lt;- make_ames()
nrow(ames)
```

```
## [1] 1607
```

```r
library(rsample)

# Make sure that you get the same random numbers
set.seed(4595)
data_split &lt;- initial_split(ames, strata = "Sale_Price")

ames_train &lt;- training(data_split)
ames_test  &lt;- testing(data_split)

nrow(ames_train)/nrow(ames)
```

```
## [1] 0.751089
```


---

# Outcome Distributions

.pull-left[

```r
library(ggplot2)
## Do the distributions line up? 
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_line(stat = "density", 
            trim = TRUE) + 
  geom_line(data = ames_test, 
            stat = "density", 
            trim = TRUE, col = "red")
```
]
.pull-right[
&lt;img src="Part_2_Basic_Principals_files/figure-html/ames-split-dists-dist-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


---

# Specifying Models in R Using Formulas

To fit a model to the housing data, the model terms must be specified. Historically, there are two main interfaces for doing this. 

The **formula** interface using R [formula rules](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models) to specify a _symbolic_ representation of the terms and variables. For example:


```r
foo(Sale_Price ~ Neighborhood + Year_Sold + Neighborhood:Year_Sold, data = ames_train)
```
or

```r
foo(log10(Sale_Price) ~ ., data = ames_train)
```
or

```r
foo(log10(Sale_Price) ~ ns(Longitude, df = 3) + ns(Latitude, df = 3), data = ames_train)
```

This is very convenient but it has some disadvantages.  


---

# Downsides to Formulas

* You can't nest in-line functions such as `foo(y ~ pca(scale(x1), scale(x2), scale(x3)), data = dat)`.

* All the model matrix calculations happen at once and can't be recycled when uses in a model function. 

* For very _wide_data sets, the formula method can be [extremely inefficient](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/). 

* There are limited _roles_ that variables can take which has led to several re-implementations of formulas. 

* Specifying multivariate outcomes 

* Not all model functions have a formula method. 


---

# Specifying Models Without Formulas

Some modeling function have the non-formula interface. This usually has arguments for the predictors and the outcome(s):


```r
# Usually, the variables must all be numeric
pre_vars &lt;- c("Year_Sold", "Longitude", "Latitude")
foo(x = ames_train[, pre_vars],
    y = ames_train$Sale_Price)
```

This is inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling. 

Overall, it is difficult to predict if a package has one or both of these interfaces. For example, `lm` only has formulas. 

There is a **third interface**, using _recipes_ that will be discussed later that solves some of these issues. 


---

# A Linear Regression Model 

Let's start by fitting an ordinary linear regression model to the training set. You can choose the model terms for your model but I will use a very simple model:


```r
simple_lm &lt;- lm(log10(Sale_Price) ~ Longitude + Latitude, data = ames_train)
```

Before looking at coefficients, we should do some model checking to see if there is anything obviously wrong with the model. 

To get the statistics on the individual data points, we will use the awesome `broom` package:


```r
library(broom)
simple_lm_values &lt;- augment(simple_lm)
names(simple_lm_values)
```

```
##  [1] "log10.Sale_Price." "Longitude"         "Latitude"         
##  [4] ".fitted"           ".se.fit"           ".resid"           
##  [7] ".hat"              ".sigma"            ".cooksd"          
## [10] ".std.resid"
```


---

# Some Basic Diagnostics 

From these results, let's take 10 minutes and do some visualizations: 

* Plot the observed versus fitted values

* Plot the residuals

* Plot the predicted versus residuals

Are there any _downsides_ to this approach? 


---

# Overall Model Statistics 

If you use the `summary` method on the `lm` object, the bottom shows some statistics: 


```r
summary(simple_lm)
```

```
## &lt;snip&gt;
## Residual standard error: 0.1625 on 1204 degrees of freedom
## Multiple R-squared:  0.1627,	Adjusted R-squared:  0.1613 
## F-statistic:   117 on 2 and 1204 DF,  p-value: &lt; 2.2e-16
```

These statistics are the result of predicting the same data that was used to derive the coefficients. This is problematic because it can lead to optimistic results, especially for models that are extremely flexible. 

The tests set is used for assessing performance. **Should we predict the test set** and use those results to estimate these statistics? 


---

&lt;img src="nope.png" width="40%" style="display: block; margin: auto;" /&gt;

(Matthew Inman/Exploding Kittens)


---

# Assessing Models

Save the test set until the very end when you have one or two models that are your favorite. 

We'll need to use the training set but....

For some models, it is possible to get very small residuals by predicting the training set. 

That's an issue since we will need to make comparisons between models, create diagnostic plots, etc. 

If only we had a method for getting honest performance estimates from the _training set_... ðŸ¤”


---

# Resampling Methods

.pull-left[
These are additional data splitting schemes that are applied to the _training_ set. 

They attempt to simulate slightly different versions of the training set. These versions of the original are split into two model subsets:

* The _analysis set_ is used to fit the model (analogous to the training set). 
* Performance is determined using the _assessment set_. 

This process is repeated many times. 

There are different flavors or resampling but we will focus on two methods. 

]
.pull-right[
<div id="htmlwidget-0570f9fdb1c01c2b2790" style="width:504px;height:504px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-0570f9fdb1c01c2b2790">{"x":{"diagram":"\ndigraph resampling_diag {\n      \ngraph [layout = dot]\n\nnode [fontname = Helvetica]\n\nall [shape = circle,\n     label = \"All\nData\"]\n\nte [shape = circle,\n    style = filled,\n    color = grey,\n    label = \"Testing\",\n    fillcolor = ivory]\n\ntr [shape = circle,\n    style = filled,\n    color = grey,\n    label = \"Training\",\n    fillcolor = honeydew]\n\nr1 [shape = rectangle,\n    label = \"Resample 1\"]\n\nan1 [shape = oval,\n     style = filled,\n     label = \"Analysis\",\n     fillcolor = honeydew]\nas1 [shape = oval,\n     style = filled,\n     label = \"Assessment\",\n     fillcolor = ivory]\n\nr2 [shape = rectangle,\n    label = \"Resample 2\"]\n      \nan2 [shape = oval,\n     style = filled,\n     label = \"Analysis\",\n     fillcolor = honeydew]\n\nas2 [shape = oval,\n     style = filled,\n     label = \"Assessment\",\n     fillcolor = ivory]\n\nr3 [shape = rectangle,\n    label = \"Resample 3\"]\n      \nan3 [shape = oval,\n     style = filled,\n     label = \"Analysis\",\n     fillcolor = honeydew]\nas3 [shape = oval,\n     style = filled,\n     label = \"Assessment\",\n     fillcolor = ivory]\n\nall -> {tr te }\ntr -> {r1 r2 r3 }\nr1 -> {as1 an1 }\nr2 -> {an2 as2 }\nr3 -> {an3 as3 }\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
]


---

# V-Fold Cross-Validation

Here, we randomly split the training data into _V_ distinct blocks of roughly equal size.

* We leave out the first block of analysis data and fit a model.

* This model is used to predict the held-out block pf assessment data.

* We continue this process until we've predicted all _V_ assessment blocks

The final performance is based on the hold-out predictions by _averaging_ the statistics from the _V_ blocks. 

_V_ is usually taken to be 5 or 10 and leave one out cross--validation has each sample as a block. 

**Repeated _V_-fold CV** creates multiple versions of the folds and aggregates the results (I prefer this method).

Leave-one-out CV has _V_ equal to the number of data points in the training set. This is largely deprecated unless you have pathologically small data sets. 


---

#  10-Fold Cross-Validation with _n_ = 50

&lt;img src="Part_2_Basic_Principals_files/figure-html/cv-plot-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Bootstrapping

A bootstrap sample is the same size as the training set but each data point is selected _with replacement_. 

This means that the analysis set will have more than one replicate of a training set instance. 

The assessment set contains all samples that were never included in the bootstrap set. It is often called the "out-of-bag" sample and can vary in size. 

On average, 63.2120559% of the training set is contained at least once in the bootstrap sample. 

---

#  Bootstrapping with _n_ = 50

&lt;img src="Part_2_Basic_Principals_files/figure-html/boot-plot-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Comparing Resampling Methods

If you think of resampling in the same manner as statistical estimators (e.g. maximum likelihood), this becomes a trade-off between bias and variance:

 * Variance is (mostly) driven by the number of resamples (e.g. 5-fold CV has larger variance than 10-fold). 
 * Bias is (mostly) related to how much data is held back. The bootstrap has large bias compared to 10-fold CV. 

There are lengthy blog posts about this subject [here](http://bit.ly/1yE0Ss5) and [here](http://bit.ly/1zfoFj2). 

I tend to favor 5 repeats of 10-fold cross-validation unless the size of the assessment data is is "large enough". 

For example, 10% of the Ames training set is 120 properties and this is probably good enough to estimate the RMSE and _R_&lt;sup&gt;2&lt;/sup&gt;.  


---

# Cross-Validating Using `rsample`


```r
set.seed(2453)
cv_splits &lt;- vfold_cv(ames_train, v = 10, strata = "Sale_Price")
cv_splits
```

```
## # 10-fold cross-validation using stratification
## # A tibble: 10 x 2
##          splits     id
##          &lt;list&gt;  &lt;chr&gt;
##  1 &lt;S3: rsplit&gt; Fold01
##  2 &lt;S3: rsplit&gt; Fold02
##  3 &lt;S3: rsplit&gt; Fold03
##  4 &lt;S3: rsplit&gt; Fold04
##  5 &lt;S3: rsplit&gt; Fold05
##  6 &lt;S3: rsplit&gt; Fold06
##  7 &lt;S3: rsplit&gt; Fold07
##  8 &lt;S3: rsplit&gt; Fold08
##  9 &lt;S3: rsplit&gt; Fold09
## 10 &lt;S3: rsplit&gt; Fold10
```

---

# Cross-Validating Using `rsample`


```r
# The `split` objects contain the information about the sample sizes
cv_splits$splits[[1]]
```

```
## &lt;1083/124/1207&gt;
```

```r
# Use the `analysis` and `assessment` functions to get the data
dim(analysis(cv_splits$splits[[1]]))
```

```
## [1] 1083   81
```

```r
dim(assessment(cv_splits$splits[[1]]))
```

```
## [1] 124  81
```


---

# Resampling the Linear Model

We'll need to write a function to fit the model to each data set and another to compute performance. 

The first argument should be the `rsplit` object contained in `cv_splits$splits`:


```r
lm_fit &lt;- function(data_split, ...) 
  lm(..., data = analysis(data_split))

model_perf &lt;- function(data_split, mod_obj) {
  assess_y &lt;- log10(assessment(data_split)$Sale_Price)
  vars &lt;- rsample::form_pred(mod_obj$terms)
  assess_x &lt;- assessment(data_split)[vars]
  assess_pred &lt;- predict(mod_obj, newdata = assessment(data_split))
  rmse &lt;- sqrt(mean((assess_pred - assess_y)^2))
  rsq &lt;- cor(assess_pred, assess_y)^2
  data.frame(RMSE = rmse, R2 = rsq)
}

# A formula is also needed for each model:
form &lt;- as.formula(log10(Sale_Price) ~ Longitude + Latitude)
```


---

# Resampling the Linear Model

The `purrr` package will be used to fit the model to each analysis set. These will be saved in a column called `lm_mod`:


```r
library(purrr)
cv_splits$lm_mod &lt;- map(cv_splits$splits, lm_fit, formula = form)
cv_splits
```

```
## # 10-fold cross-validation using stratification
## # A tibble: 10 x 3
##          splits     id   lm_mod
##          &lt;list&gt;  &lt;chr&gt;   &lt;list&gt;
##  1 &lt;S3: rsplit&gt; Fold01 &lt;S3: lm&gt;
##  2 &lt;S3: rsplit&gt; Fold02 &lt;S3: lm&gt;
##  3 &lt;S3: rsplit&gt; Fold03 &lt;S3: lm&gt;
##  4 &lt;S3: rsplit&gt; Fold04 &lt;S3: lm&gt;
##  5 &lt;S3: rsplit&gt; Fold05 &lt;S3: lm&gt;
##  6 &lt;S3: rsplit&gt; Fold06 &lt;S3: lm&gt;
##  7 &lt;S3: rsplit&gt; Fold07 &lt;S3: lm&gt;
##  8 &lt;S3: rsplit&gt; Fold08 &lt;S3: lm&gt;
##  9 &lt;S3: rsplit&gt; Fold09 &lt;S3: lm&gt;
## 10 &lt;S3: rsplit&gt; Fold10 &lt;S3: lm&gt;
```

---

# Resampling the Linear Model

Now, let's compute the two performance measures:


```r
# map2 can be used to move over two objects of equal length
lm_res &lt;- map2_df(cv_splits$splits, cv_splits$lm_mod, model_perf) %&gt;% 
  rename(RMSE_simple = RMSE, R2_simple = R2)
head(lm_res, 3)
```

```
##   RMSE_simple  R2_simple
## 1   0.1441513 0.24075121
## 2   0.1707427 0.12928952
## 3   0.1511241 0.08276535
```

```r
## Merge in results:
cv_splits &lt;- cv_splits %&gt;% bind_cols(lm_res)

## Rename the columns and compute the resampling estimates:
cv_splits %&gt;% select(RMSE_simple, R2_simple) %&gt;% colMeans
```

```
## RMSE_simple   R2_simple 
##   0.1624556   0.1696015
```


---

# What Was the Ruckus?

Previously, I mentioned that the performance metrics that were naively calculated from the training set could be optimistic. However, this approach estimates the RMSE to be 0.1625 and cross-validation produced an estimate of 0.1625. What was the big deal? 

Linear regression is a _high bias model_. This means that it is fairly incapable at being able to adapt the underlying model function (unless it is linear). For this reason, linear regression is unlikely to **overfit** to the training set and our two estimates are likely to be the same. 

We'll consider another model shortly that is _low bias_ since it can, theoretically, easily adapt to a wide variety of true model functions. 

However, as before, there is also variance to consider. Linear regression is very stable since it leverages all of the data points to estimate parameters. Other methods, such as tree-based models, are not and can drastically change if the training set data is slightly perturbed. 

**tl;dr**: the earlier concern is real but linear regression is less likely to be affected.  

---

# Diagnostics Again

Now let's look at diagnostics using the predictions from the assessment sets. 


```r
get_assessment &lt;- function(splits, model) {
  res &lt;- augment(model, newdata = assessment(splits))
  res$.resid &lt;- log10(assessment(splits)$Sale_Price) - res$.fitted
  res
}

oob_results &lt;- map2_df(cv_splits$splits, cv_splits$lm_mod, get_assessment)
dim(oob_results)
```

```
## [1] 1207   84
```

```r
dim(ames_train)
```

```
## [1] 1207   81
```


---

# _K_-Nearest Neighbors Model

The model stores the training set (including the outcome). 

When a new sample is predicted, _K_ training set points are found that are most similar to the new sample being predicted. 

The predicted value for the new sample is some summary statistic of the neighbors, usually: 

* the mean for regression, or
* the mode for classification.

When _K_ is small, the model is very responsive to the underlying data. When _K_ is large, it begins to "over smooth" the neighbors and performance suffers. 


---

# _K_-Nearest Neighbors Model

<div id="htmlwidget-88436c317a428b935d1a" style="width:504px;height:504px;" class="leaflet html-widget"></div>
<script type="application/json" data-for="htmlwidget-88436c317a428b935d1a">{"x":{"options":{"crs":{"crsClass":"L.CRS.EPSG3857","code":null,"proj4def":null,"projectedBounds":null,"options":{}}},"setView":[[42.05848,-93.636947],16,[]],"calls":[{"method":"addProviderTiles","args":["CartoDB.Positron",null,null,{"errorTileUrl":"","noWrap":false,"zIndex":null,"unloadInvisibleTiles":null,"updateWhenIdle":null,"detectRetina":false,"reuseTiles":false}]},{"method":"addCircles","args":[[42.058421,42.058274,42.058059,42.058194,42.057854],[-93.636949,-93.636955,-93.637046,-93.635824,-93.636047],10,null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"red","weight":5,"opacity":0.4,"fill":true,"fillColor":"red","fillOpacity":0.2,"dashArray":null},null,null,null,null,null,null]},{"method":"addProviderTiles","args":["CartoDB.Positron",null,null,{"errorTileUrl":"","noWrap":false,"zIndex":null,"unloadInvisibleTiles":null,"updateWhenIdle":null,"detectRetina":false,"reuseTiles":false}]},{"method":"addCircles","args":[42.05848,-93.636947,10,null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"blue","weight":5,"opacity":0.4,"fill":true,"fillColor":"blue","fillOpacity":0.2,"dashArray":null},null,null,null,null,null,null]},{"method":"addCircles","args":[[42.054035,42.052681,42.060899,42.060779,42.062978,42.060728,42.06112,42.059193,42.058151,42.059169,42.061239,42.060068,42.05885,42.056673,42.05027,42.049297,42.055147,42.054592,42.055227,42.053395,42.05677,42.055318,42.051685,42.049672,42.050681,42.049811,42.061607,42.060502,42.060879,42.060298,42.059617,42.058388,42.058599,42.057102,42.057164,42.057181,42.06257,42.061443,42.060977,42.061193,42.060936,42.057103,42.054386,42.051721,42.051674,42.052457,42.052437,42.050909,42.05306,42.051827,42.051653,42.050084,42.052347,42.051429,42.036255,42.036113,42.03548,42.036633,42.034555,42.034678,42.0362,42.035685,42.034856,42.03457,42.034603,42.048253,42.046825,42.047452,42.047899,42.047876,42.047782,42.047008,42.046145,42.046145,42.046424,42.046114,42.047408,42.04638,42.042565,42.043704,42.048606,42.046322,42.043038,42.043306,42.043306,42.043329,42.043821,42.046574,42.042205,42.043363,42.039085,42.040326,42.041146,42.040391,42.042011,42.039211,42.039211,42.038538,42.035256,42.035482,42.036095,42.037424,42.045815,42.047447,42.043984,42.042908,42.042523,42.044847,42.043969,42.044052,42.044176,42.044883,42.043772,42.04137,42.038515,42.039841,42.038292,42.038249,42.038248,42.035869,42.040085,42.035867,42.03594,42.034618,42.034848,42.034748,42.034851,42.036873,42.037093,42.033313,42.032134,42.031363,42.031381,42.033279,42.033644,42.032391,42.03128,42.030383,42.030467,42.030388,42.028099,42.029146,42.028083,42.03005,42.028164,42.033642,42.030187,42.033621,42.033563,42.0336,42.033665,42.033631,42.031337,42.032302,42.028888,42.028875,42.020693,42.026741,42.033454,42.032432,42.026752,42.02616,42.022825,42.033387,42.03237,42.029867,42.029924,42.029731,42.03136,42.03128,42.030082,42.03009,42.03387,42.032164,42.026547,42.048514,42.023079,42.028226,42.026533,42.024208,42.025012,42.025045,42.024126,42.025019,42.024162,42.033783,42.03068,42.030599,42.025747,42.024614,42.030601,42.029011,42.024318,42.021321,42.022045,42.018662,42.018707,42.016121,42.018721,42.017749,42.016275,42.016253,42.014375,42.014092,42.014023,42.021041,42.020635,42.020306,42.019849,42.01883,42.021793,42.021583,42.017726,42.022674,42.016244,42.019091,42.019093,42.014682,42.009345,42.009337,42.010797,42.022772,42.021414,42.021456,42.022653,42.02152,42.008375,41.99706,41.997052,41.996065,41.994614,41.994522,41.998382,41.995346,41.994284,41.995835,41.995489,41.99435,41.992989,41.99321,41.990934,41.991004,41.991861,41.991758,41.991876,41.992212,41.9917,41.988729,41.986647,41.991367,41.989185,42.052758,42.05139,42.049322,42.059956,42.063294,42.060896,42.062956,42.060598,42.062903,42.061633,42.061178,42.059283,42.059129,42.059161,42.059645,42.058201,42.059242,42.060265,42.059817,42.058952,42.055408,42.055081,42.054625,42.052507,42.052102,42.052708,42.049406,42.052448,42.052356,42.056557,42.055228,42.055184,42.056751,42.056771,42.052548,42.051683,42.051822,42.050705,42.050682,42.050533,42.050213,42.05009,42.050222,42.050397,42.0489,42.048486,42.061485,42.063057,42.063025,42.06128,42.062991,42.062992,42.062942,42.062257,42.063057,42.063057,42.062167,42.062108,42.060353,42.060576,42.060575,42.060426,42.06181,42.06181,42.061404,42.059511,42.058782,42.058806,42.059177,42.059179,42.059183,42.058339,42.057177,42.05807,42.057164,42.057089,42.057019,42.056432,42.057277,42.057467,42.063183,42.062404,42.062562,42.062557,42.062081,42.061562,42.059377,42.061739,42.06132,42.061114,42.061656,42.061365,42.063283,42.058368,42.05859,42.059481,42.059068,42.0595,42.057834,42.057833,42.057098,42.057093,42.056955,42.058668,42.053911,42.053441,42.053041,42.052333,42.050411,42.04992,42.049921,42.050971,42.054282,42.054302,42.054152,42.054151,42.053212,42.053258,42.053108,42.051804,42.051797,42.051793,42.051795,42.052127,42.05142,42.051378,42.052122,42.051414,42.051247,42.051251,42.050895,42.037909,42.037916,42.037923,42.036001,42.037773,42.037756,42.037773,42.03607,42.036905,42.036576,42.035518,42.037607,42.035968,42.035287,42.034747,42.034615,42.035289,42.03466,42.035331,42.034595,42.034602,42.034581,42.037236,42.047669,42.048416,42.048488,42.047008,42.047008,42.047101,42.046144,42.043782,42.042652,42.043553,42.043019,42.034509,42.047708,42.048009,42.04896,42.047652,42.046476,42.045653,42.045651,42.043842,42.044693,42.048796,42.046635,42.043935,42.043145,42.043135,42.044843,42.044774,42.044926,42.04488,42.040483,42.039902,42.040888,42.041104,42.039463,42.038406,42.035489,42.036182,42.034741,42.037452,42.049306,42.046827,42.048608,42.048724,42.04806,42.04579,42.043966,42.044125,42.043991,42.044914,42.04297,42.042233,42.043046,42.040857,42.040331,42.038397,42.038356,42.038398,42.037122,42.037109,42.034918,42.034686,42.034553,42.036223,42.034746,42.038228,42.037001,42.037144,42.037328,42.036015,42.036002,42.034668,42.034627,42.034628,42.03528,42.041945,42.040295,42.04124,42.040766,42.038811,42.037129,42.035712,42.035841,42.037233,42.03602,42.035968,42.035706,42.037214,42.034622,42.034858,42.037049,42.038257,42.035287,42.034391,42.033273,42.0322,42.032203,42.032396,42.032108,42.031305,42.030275,42.030282,42.033579,42.032048,42.032301,42.032227,42.030523,42.030196,42.029007,42.028972,42.029305,42.028104,42.028072,42.027062,42.0272,42.033752,42.033558,42.033268,42.031542,42.033406,42.03329,42.033347,42.033335,42.031395,42.03252,42.03246,42.031292,42.032375,42.03235,42.031203,42.02843,42.026759,42.028948,42.027039,42.023085,42.024053,42.024143,42.022828,42.032681,42.028049,42.027493,42.022978,42.033376,42.033633,42.032367,42.032388,42.029863,42.030628,42.031138,42.033427,42.032302,42.032503,42.023229,42.024203,42.028468,42.027506,42.024483,42.023982,42.023916,42.025452,42.025356,42.026307,42.024993,42.024871,42.02487,42.023548,42.034545,42.032791,42.031805,42.03254,42.030625,42.030623,42.030678,42.03252,42.031622,42.031545,42.031419,42.03057,42.030531,42.025707,42.025293,42.024628,42.024596,42.024613,42.024537,42.024633,42.024399,42.025209,42.021176,42.021207,42.020817,42.019784,42.018597,42.017569,42.017228,42.018667,42.016399,42.016551,42.01613,42.015637,42.021384,42.020087,42.019184,42.019697,42.019548,42.019691,42.020666,42.020991,42.021788,42.018722,42.018847,42.01877,42.018703,42.016973,42.016978,42.017996,42.018875,42.017856,42.016819,42.016797,42.016816,42.016642,42.018973,42.018965,42.020062,42.020504,42.022655,42.021436,42.017962,42.01965,42.01873,42.019937,42.01902,42.018761,42.01804,42.018039,42.017888,42.017889,42.019256,42.017103,42.017106,42.016115,42.016216,42.016217,42.016217,42.016853,42.016067,42.020145,42.020223,42.018922,42.017572,42.020037,42.018504,42.018941,42.017051,42.017573,42.013003,42.015455,42.012488,42.011506,42.011836,42.009321,42.011343,42.010076,42.022836,42.022474,42.022658,42.022658,42.022685,42.0228,42.021547,42.021278,42.020161,42.020077,42.021556,42.021555,42.009401,42.009401,41.994357,41.995218,41.995973,41.995931,41.995512,41.994522,42.005692,42.000503,41.994241,41.994137,41.994824,41.995088,41.998023,41.997992,41.997364,41.995508,41.997299,41.997352,41.993288,41.992975,41.993261,41.990054,41.993356,41.993107,41.993192,41.991211,41.992917,41.99284,41.992516,41.992479,41.991709,41.991641,41.991868,41.991612,41.991098,41.989981,41.989488,41.991144,41.991644,41.989905,41.987606,41.989981,41.98822,41.98651,41.986588,42.053801,42.052519,42.050507,42.059909,42.061407,42.060425,42.06048,42.060736,42.061879,42.06281,42.062802,42.062798,42.062956,42.060625,42.0591,42.058201,42.059774,42.059332,42.058136,42.060527,42.059957,42.061096,42.056702,42.058966,42.059947,42.055607,42.054367,42.054956,42.056399,42.05327,42.05255,42.053418,42.049771,42.051077,42.052362,42.052359,42.04977,42.053459,42.054385,42.055319,42.0524,42.052447,42.052421,42.051798,42.051839,42.051691,42.051672,42.051673,42.051664,42.050595,42.048946,42.050705,42.062167,42.061284,42.063057,42.062109,42.061134,42.060355,42.060272,42.060271,42.060502,42.060577,42.060866,42.060502,42.061377,42.059636,42.06181,42.05869,42.059216,42.05869,42.059184,42.057164,42.058231,42.058238,42.057164,42.057164,42.05758,42.063304,42.062442,42.062361,42.063031,42.062576,42.061984,42.061987,42.063185,42.062374,42.062549,42.063301,42.063301,42.061464,42.060293,42.061727,42.060836,42.05865,42.05853,42.057905,42.057838,42.05696,42.05384,42.053801,42.053314,42.05178,42.051073,42.049687,42.049528,42.055056,42.054176,42.054175,42.053183,42.053033,42.0518,42.052265,42.052272,42.052346,42.051355,42.051426,42.051382,42.051245,42.051224,42.037843,42.037827,42.036605,42.03718,42.036849,42.034595,42.03674,42.035555,42.035629,42.03748,42.035235,42.036051,42.036087,42.036206,42.035302,42.034692,42.035696,42.035841,42.035325,42.035536,42.03457,42.037065,42.045763,42.048608,42.048371,42.047997,42.047874,42.047592,42.04768,42.047665,42.047642,42.047618,42.046145,42.046107,42.046429,42.046416,42.044609,42.043779,42.042369,42.042826,42.045632,42.048997,42.048602,42.046472,42.04806,42.044669,42.046496,42.045706,42.045728,42.045728,42.045727,42.048253,42.04657,42.043214,42.042114,42.040072,42.038434,42.04102,42.041154,42.041314,42.040273,42.040143,42.039195,42.038351,42.039202,42.035836,42.035379,42.035666,42.038213,42.034612,42.036024,42.034831,42.035987,42.037247,42.0493,42.048528,42.042152,42.042156,42.043129,42.044735,42.043897,42.04498,42.044741,42.043814,42.042105,42.044018,42.042896,42.042905,42.042001,42.040745,42.039447,42.038357,42.039387,42.03837,42.03837,42.038836,42.040276,42.039351,42.034617,42.037086,42.037001,42.037097,42.037041,42.0358,42.036949,42.035857,42.035277,42.034761,42.040811,42.040105,42.040081,42.040297,42.040148,42.038343,42.041871,42.040768,42.03902,42.036098,42.035782,42.035996,42.036099,42.036053,42.034805,42.03471,42.034662,42.034829,42.034757,42.034708,42.036792,42.03548,42.034614,42.033267,42.033324,42.033385,42.032144,42.032346,42.03139,42.031361,42.03121,42.030396,42.030463,42.031196,42.030267,42.032064,42.031964,42.030368,42.03146,42.031138,42.029142,42.027993,42.029136,42.028951,42.027953,42.029025,42.027118,42.02297,42.03405,42.03179,42.030786,42.033306,42.033369,42.032299,42.031499,42.032396,42.032478,42.031415,42.03118,42.030389,42.02686,42.028544,42.026886,42.02689,42.025258,42.02511,42.023964,42.034411,42.034576,42.033432,42.027689,42.028002,42.027997,42.033521,42.03224,42.032304,42.032383,42.031356,42.030611,42.030702,42.031362,42.031347,42.030231,42.030232,42.033285,42.03247,42.032307,42.033686,42.032478,42.031635,42.032157,42.032575,42.029453,42.028836,42.023651,42.02653,42.027076,42.026614,42.02821,42.028191,42.025102,42.024907,42.022824,42.032082,42.032493,42.030933,42.031496,42.024601,42.024635,42.024555,42.024615,42.025687,42.025218,42.025194,42.024567,42.024481,42.023306,42.024567,42.023308,42.023138,42.029884,42.028168,42.025182,42.02438,42.022227,42.021209,42.0209,42.019047,42.018043,42.018123,42.017221,42.017929,42.01686,42.021388,42.019697,42.021811,42.021056,42.021785,42.01834,42.018664,42.017854,42.01791,42.016979,42.01811,42.017867,42.017858,42.017186,42.016252,42.016618,42.016092,42.014636,42.014109,42.014092,42.014092,42.01397,42.013991,42.014758,42.013519,42.014593,42.020995,42.020201,42.020201,42.021113,42.01899,42.021168,42.018988,42.018961,42.01896,42.018959,42.018813,42.018812,42.01881,42.01881,42.018809,42.02011,42.022659,42.021432,42.018848,42.017727,42.022089,42.022088,42.02197,42.020286,42.018837,42.01924,42.019093,42.01895,42.018857,42.016941,42.016086,42.016072,42.017609,42.01874,42.019059,42.019759,42.016827,42.016385,42.017589,42.016039,42.013296,42.013495,42.014936,42.022786,42.022709,42.020441,42.020855,42.020207,42.019025,42.019098,42.019099,42.019099,42.022603,42.009398,42.008375,41.996849,41.994357,41.995987,41.996907,41.995702,41.996063,42.004509,41.999553,41.998023,41.996331,41.994347,41.994229,41.993258,41.993369,41.99329,41.993173,41.990054,41.990993,41.993062,41.991574,41.992159,41.991708,41.992522,41.992497,41.99171,41.989769,41.990134,41.988609,41.987686,41.988964,41.98651,41.990921,41.989265],[-93.619754,-93.618672,-93.638933,-93.638925,-93.633792,-93.633826,-93.632852,-93.639068,-93.638647,-93.632913,-93.62655,-93.622857,-93.628804,-93.622971,-93.636372,-93.639366,-93.626231,-93.626537,-93.628806,-93.627112,-93.622295,-93.624373,-93.62728,-93.627232,-93.625966,-93.625848,-93.652561,-93.654606,-93.652336,-93.652307,-93.655051,-93.650436,-93.656067,-93.654853,-93.654121,-93.651013,-93.641635,-93.644076,-93.641487,-93.642447,-93.641325,-93.641337,-93.651381,-93.654206,-93.653616,-93.650356,-93.650578,-93.652972,-93.642368,-93.649976,-93.650318,-93.647105,-93.642773,-93.642224,-93.692309,-93.692412,-93.686417,-93.68628,-93.685028,-93.685029,-93.677085,-93.670956,-93.672096,-93.672067,-93.660672,-93.646749,-93.651773,-93.647988,-93.644889,-93.644889,-93.64489,-93.644813,-93.645606,-93.645585,-93.645482,-93.645478,-93.641153,-93.643556,-93.649361,-93.648143,-93.639516,-93.633351,-93.636879,-93.637463,-93.637531,-93.637788,-93.632554,-93.626137,-93.625926,-93.623383,-93.626939,-93.626785,-93.627809,-93.625728,-93.622421,-93.622864,-93.620651,-93.620504,-93.629287,-93.629313,-93.624601,-93.621431,-93.617882,-93.614244,-93.617553,-93.617117,-93.617328,-93.618626,-93.616582,-93.614882,-93.613301,-93.61513,-93.614509,-93.617092,-93.61722,-93.618428,-93.611531,-93.618788,-93.61873,-93.620341,-93.607275,-93.61061,-93.605969,-93.61026,-93.607801,-93.605943,-93.606742,-93.604835,-93.603705,-93.617014,-93.617135,-93.62024,-93.620241,-93.614002,-93.615431,-93.615446,-93.608954,-93.610466,-93.610468,-93.606789,-93.616887,-93.610588,-93.608878,-93.608246,-93.607238,-93.625737,-93.625825,-93.625586,-93.624566,-93.622597,-93.623523,-93.622448,-93.621482,-93.620409,-93.621674,-93.621835,-93.629495,-93.620386,-93.656231,-93.655538,-93.656924,-93.658081,-93.655849,-93.671076,-93.672381,-93.678141,-93.677992,-93.676654,-93.673565,-93.672523,-93.670503,-93.670114,-93.663619,-93.668378,-93.667904,-93.65041,-93.671178,-93.662762,-93.66357,-93.667708,-93.664908,-93.666245,-93.665799,-93.663367,-93.659808,-93.683167,-93.682026,-93.679781,-93.69019,-93.691269,-93.68329,-93.679735,-93.682138,-93.69188,-93.687807,-93.687859,-93.684454,-93.687764,-93.680864,-93.682686,-93.681227,-93.6837,-93.687749,-93.684015,-93.685961,-93.677553,-93.677545,-93.675268,-93.675175,-93.666164,-93.664841,-93.660168,-93.661944,-93.655762,-93.650155,-93.64678,-93.646683,-93.642793,-93.645701,-93.645729,-93.641677,-93.625291,-93.629496,-93.615272,-93.606593,-93.60775,-93.616242,-93.604947,-93.602823,-93.603972,-93.601489,-93.601565,-93.657885,-93.649203,-93.649096,-93.64913,-93.647645,-93.646814,-93.652495,-93.60743,-93.608372,-93.608223,-93.60359,-93.604318,-93.604425,-93.603435,-93.601806,-93.606856,-93.600582,-93.6001,-93.600147,-93.618526,-93.619562,-93.615524,-93.639388,-93.639915,-93.6364,-93.635838,-93.637442,-93.634265,-93.633547,-93.633024,-93.63921,-93.639069,-93.639069,-93.637671,-93.639325,-93.633245,-93.627751,-93.628639,-93.629734,-93.636512,-93.635056,-93.632585,-93.63745,-93.638366,-93.633793,-93.639366,-93.630253,-93.632221,-93.625015,-93.628707,-93.626603,-93.622722,-93.622276,-93.627614,-93.627565,-93.629881,-93.624748,-93.625945,-93.625918,-93.625846,-93.625688,-93.625696,-93.625707,-93.625174,-93.626521,-93.658585,-93.655081,-93.652716,-93.657483,-93.654241,-93.654144,-93.652863,-93.654567,-93.655081,-93.655081,-93.655972,-93.654437,-93.656737,-93.654724,-93.65476,-93.654652,-93.64287,-93.64287,-93.643771,-93.65827,-93.65486,-93.654787,-93.649779,-93.649743,-93.649723,-93.650258,-93.655483,-93.654918,-93.654204,-93.654215,-93.654405,-93.651001,-93.651014,-93.650238,-93.64466,-93.645627,-93.641561,-93.641515,-93.642911,-93.642749,-93.643312,-93.644106,-93.641416,-93.641488,-93.642626,-93.640051,-93.640065,-93.649649,-93.642813,-93.642932,-93.641919,-93.643756,-93.641313,-93.641303,-93.641367,-93.641397,-93.641312,-93.640879,-93.657163,-93.652608,-93.65238,-93.655426,-93.655988,-93.651992,-93.650529,-93.650822,-93.644121,-93.642553,-93.643944,-93.642579,-93.64236,-93.641251,-93.641296,-93.650321,-93.6504,-93.65044,-93.648778,-93.643842,-93.642474,-93.643752,-93.640847,-93.641746,-93.643131,-93.641501,-93.639832,-93.691737,-93.691792,-93.691874,-93.69143,-93.69117,-93.690159,-93.69117,-93.69139,-93.6888,-93.687843,-93.686267,-93.687804,-93.685037,-93.680198,-93.677008,-93.674051,-93.675716,-93.669554,-93.669538,-93.673518,-93.669707,-93.669845,-93.660327,-93.650333,-93.646316,-93.646338,-93.644813,-93.644813,-93.644205,-93.64566,-93.645923,-93.649358,-93.648057,-93.645546,-93.65921,-93.635342,-93.634634,-93.634803,-93.631519,-93.631851,-93.632432,-93.632504,-93.637905,-93.631899,-93.628903,-93.624612,-93.627416,-93.625775,-93.622991,-93.623485,-93.622635,-93.622476,-93.623411,-93.626784,-93.627985,-93.626979,-93.624581,-93.622685,-93.621529,-93.629499,-93.62568,-93.623705,-93.624612,-93.619673,-93.615686,-93.615602,-93.615558,-93.614619,-93.614216,-93.618784,-93.61864,-93.61488,-93.614059,-93.614768,-93.61588,-93.613557,-93.618478,-93.615763,-93.61867,-93.617219,-93.6124,-93.617207,-93.615848,-93.618605,-93.618607,-93.618997,-93.620336,-93.617182,-93.614177,-93.613965,-93.61239,-93.612241,-93.614049,-93.615465,-93.614049,-93.61087,-93.61097,-93.612254,-93.610691,-93.610588,-93.610649,-93.606573,-93.60552,-93.609059,-93.608977,-93.608903,-93.60891,-93.606891,-93.606889,-93.605968,-93.606786,-93.610491,-93.610605,-93.604987,-93.603879,-93.603701,-93.617306,-93.615583,-93.618588,-93.618734,-93.62026,-93.615599,-93.618486,-93.618981,-93.615381,-93.607776,-93.608829,-93.606842,-93.606841,-93.610469,-93.606788,-93.617054,-93.61713,-93.612245,-93.610582,-93.608604,-93.608704,-93.608707,-93.628642,-93.628848,-93.627323,-93.626116,-93.624564,-93.622513,-93.623515,-93.621469,-93.624689,-93.623645,-93.623644,-93.623629,-93.622565,-93.621351,-93.620482,-93.626994,-93.625362,-93.621525,-93.621635,-93.626716,-93.624048,-93.625154,-93.599575,-93.659205,-93.657107,-93.660349,-93.658381,-93.669839,-93.675401,-93.672288,-93.673382,-93.677991,-93.671849,-93.671369,-93.666432,-93.666813,-93.660877,-93.671177,-93.672276,-93.666242,-93.665994,-93.667776,-93.668212,-93.667209,-93.664902,-93.664902,-93.660147,-93.663366,-93.663505,-93.660581,-93.661005,-93.684025,-93.685539,-93.685536,-93.68341,-93.681063,-93.682025,-93.682176,-93.679789,-93.679784,-93.679784,-93.679933,-93.679781,-93.679856,-93.690266,-93.690971,-93.689072,-93.689071,-93.691433,-93.691481,-93.690099,-93.689066,-93.682323,-93.692135,-93.688372,-93.690401,-93.691355,-93.690548,-93.690324,-93.691817,-93.688383,-93.691175,-93.689942,-93.691672,-93.689028,-93.686916,-93.687212,-93.687219,-93.685068,-93.684887,-93.684954,-93.683157,-93.683159,-93.68317,-93.686218,-93.68269,-93.682623,-93.682983,-93.686744,-93.685673,-93.679571,-93.680706,-93.681411,-93.68218,-93.68167,-93.677538,-93.676241,-93.670587,-93.665737,-93.664768,-93.664686,-93.661542,-93.660303,-93.665875,-93.664821,-93.663455,-93.663329,-93.650162,-93.65181,-93.646624,-93.646688,-93.646817,-93.64672,-93.644926,-93.644544,-93.644669,-93.644219,-93.644511,-93.644698,-93.64476,-93.646289,-93.646416,-93.643705,-93.643678,-93.64236,-93.641948,-93.644426,-93.641322,-93.642359,-93.64269,-93.640497,-93.642883,-93.644366,-93.640326,-93.641849,-93.644923,-93.645785,-93.643548,-93.640182,-93.628412,-93.6295,-93.596569,-93.597005,-93.62826,-93.625313,-93.629496,-93.628236,-93.628222,-93.616969,-93.614021,-93.613886,-93.615697,-93.615618,-93.606577,-93.607551,-93.606933,-93.602505,-93.60104,-93.601565,-93.63966,-93.649815,-93.650588,-93.650569,-93.651732,-93.650417,-93.646617,-93.644366,-93.646782,-93.646894,-93.646634,-93.662162,-93.653109,-93.652429,-93.647569,-93.610102,-93.608192,-93.605507,-93.607623,-93.607395,-93.605355,-93.605354,-93.601862,-93.601827,-93.601507,-93.602094,-93.603422,-93.604923,-93.604644,-93.606778,-93.604524,-93.602559,-93.601162,-93.607114,-93.604858,-93.606778,-93.604146,-93.606641,-93.604858,-93.618543,-93.616439,-93.617432,-93.639067,-93.637871,-93.637482,-93.637566,-93.636378,-93.636373,-93.635967,-93.635935,-93.635837,-93.635932,-93.631604,-93.635716,-93.639293,-93.634613,-93.631572,-93.632571,-93.629337,-93.626337,-93.626477,-93.625169,-93.629659,-93.624062,-93.638026,-93.637874,-93.631515,-93.633622,-93.635486,-93.634036,-93.636044,-93.635367,-93.639516,-93.632369,-93.632355,-93.630289,-93.625373,-93.627849,-93.624411,-93.62977,-93.629272,-93.627749,-93.627103,-93.627328,-93.627366,-93.629877,-93.629846,-93.629516,-93.627169,-93.62895,-93.624767,-93.655972,-93.657991,-93.655081,-93.654774,-93.657984,-93.658432,-93.657118,-93.657149,-93.654606,-93.654642,-93.652334,-93.654606,-93.643862,-93.654997,-93.64287,-93.653084,-93.650626,-93.653084,-93.649714,-93.654138,-93.649553,-93.649525,-93.654124,-93.65414,-93.650617,-93.646459,-93.645841,-93.645774,-93.644627,-93.645755,-93.644213,-93.644306,-93.640227,-93.641321,-93.641475,-93.642261,-93.642116,-93.643971,-93.642588,-93.644094,-93.641467,-93.642713,-93.642913,-93.641281,-93.64136,-93.641283,-93.654519,-93.654444,-93.657851,-93.653915,-93.655704,-93.653608,-93.653479,-93.642317,-93.64178,-93.641793,-93.643635,-93.643811,-93.650361,-93.64407,-93.643898,-93.642862,-93.642309,-93.642374,-93.64357,-93.643173,-93.643977,-93.692575,-93.690959,-93.691246,-93.691247,-93.689021,-93.68782,-93.687695,-93.686267,-93.686267,-93.68665,-93.681407,-93.674167,-93.675963,-93.677123,-93.675723,-93.669553,-93.671037,-93.672411,-93.670919,-93.670998,-93.669708,-93.660643,-93.653876,-93.653512,-93.652348,-93.650474,-93.650397,-93.647832,-93.646526,-93.644891,-93.644891,-93.644891,-93.645617,-93.645478,-93.643373,-93.64114,-93.648931,-93.646065,-93.649329,-93.649352,-93.633752,-93.634738,-93.635873,-93.633466,-93.630258,-93.633098,-93.627935,-93.625976,-93.626161,-93.62629,-93.626604,-93.626606,-93.626345,-93.623414,-93.622312,-93.630156,-93.626765,-93.627065,-93.628097,-93.621621,-93.621362,-93.622017,-93.62357,-93.622649,-93.621399,-93.628568,-93.629256,-93.628176,-93.626875,-93.624738,-93.623704,-93.620494,-93.621426,-93.62143,-93.619673,-93.615629,-93.617334,-93.620355,-93.618389,-93.619334,-93.617499,-93.618625,-93.612463,-93.612406,-93.615882,-93.613078,-93.613521,-93.612462,-93.617374,-93.618481,-93.617225,-93.617069,-93.612464,-93.612575,-93.612626,-93.615474,-93.61547,-93.612263,-93.617182,-93.612522,-93.613965,-93.610772,-93.612239,-93.6139,-93.610896,-93.612266,-93.612404,-93.613899,-93.608984,-93.607582,-93.60942,-93.610278,-93.610501,-93.607793,-93.606409,-93.60674,-93.605727,-93.609054,-93.610503,-93.608903,-93.608904,-93.606743,-93.609053,-93.609053,-93.609053,-93.606892,-93.606893,-93.606744,-93.603704,-93.605839,-93.605755,-93.618539,-93.618751,-93.617164,-93.61852,-93.617139,-93.618489,-93.618682,-93.618799,-93.618335,-93.618334,-93.615638,-93.613905,-93.607862,-93.610354,-93.608938,-93.607744,-93.606803,-93.618293,-93.616886,-93.613889,-93.615361,-93.615337,-93.610438,-93.606923,-93.606869,-93.628478,-93.626022,-93.627385,-93.625576,-93.620433,-93.623642,-93.62454,-93.624553,-93.624553,-93.623481,-93.624388,-93.622386,-93.622773,-93.625436,-93.621562,-93.622624,-93.626858,-93.626979,-93.625518,-93.657017,-93.65565,-93.657945,-93.660351,-93.655885,-93.655786,-93.67555,-93.674456,-93.674517,-93.675408,-93.675606,-93.676455,-93.674385,-93.673601,-93.672645,-93.670204,-93.670514,-93.665569,-93.665505,-93.667081,-93.669544,-93.66412,-93.668392,-93.667082,-93.662039,-93.678165,-93.676352,-93.674402,-93.666035,-93.664908,-93.663475,-93.661562,-93.660664,-93.666095,-93.661904,-93.658506,-93.685537,-93.683235,-93.68218,-93.679783,-93.689071,-93.689816,-93.690366,-93.691234,-93.688937,-93.688925,-93.688924,-93.688995,-93.689067,-93.689084,-93.688995,-93.688934,-93.688929,-93.68216,-93.681349,-93.682319,-93.683877,-93.691896,-93.688302,-93.690363,-93.692005,-93.692606,-93.692221,-93.691895,-93.689083,-93.68923,-93.686629,-93.684982,-93.683468,-93.683234,-93.681378,-93.687703,-93.686288,-93.684421,-93.685488,-93.685571,-93.67965,-93.679803,-93.682686,-93.681895,-93.683728,-93.683777,-93.687685,-93.687826,-93.684089,-93.684015,-93.684015,-93.685369,-93.685872,-93.683796,-93.684332,-93.683758,-93.676369,-93.675263,-93.675263,-93.671454,-93.672379,-93.671304,-93.672226,-93.670025,-93.669977,-93.669927,-93.670063,-93.670039,-93.669985,-93.66996,-93.669924,-93.661951,-93.659222,-93.660119,-93.663458,-93.661995,-93.656753,-93.6567,-93.65564,-93.648562,-93.651811,-93.646942,-93.646619,-93.646438,-93.648416,-93.64644,-93.648415,-93.647027,-93.644307,-93.644258,-93.642431,-93.64048,-93.641155,-93.642319,-93.641002,-93.641511,-93.642884,-93.644203,-93.639622,-93.628467,-93.62841,-93.628298,-93.629496,-93.626683,-93.615307,-93.614965,-93.615012,-93.615024,-93.604344,-93.616026,-93.616242,-93.599969,-93.606736,-93.603671,-93.603173,-93.601191,-93.603854,-93.644527,-93.646099,-93.646575,-93.646643,-93.646758,-93.64789,-93.64898,-93.64653,-93.608343,-93.608195,-93.607142,-93.604391,-93.604279,-93.601089,-93.603524,-93.600439,-93.6019,-93.601844,-93.601615,-93.604796,-93.603534,-93.608214,-93.606842,-93.604776,-93.606847,-93.60019,-93.599996],6,null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"grey","weight":5,"opacity":0.4,"fill":true,"fillColor":"grey","fillOpacity":0.2,"dashArray":null},null,null,null,null,null,null]},{"method":"addPolylines","args":[[[[{"lng":[-93.636947,-93.636949],"lat":[42.05848,42.058421]}]]],null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"#03F","weight":1,"opacity":0.5,"fill":false,"fillColor":"#03F","fillOpacity":0.2,"dashArray":null,"smoothFactor":1,"noClip":false},null,null,null,null,null]},{"method":"addPolylines","args":[[[[{"lng":[-93.636947,-93.636955],"lat":[42.05848,42.058274]}]]],null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"#03F","weight":1,"opacity":0.5,"fill":false,"fillColor":"#03F","fillOpacity":0.2,"dashArray":null,"smoothFactor":1,"noClip":false},null,null,null,null,null]},{"method":"addPolylines","args":[[[[{"lng":[-93.636947,-93.637046],"lat":[42.05848,42.058059]}]]],null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"#03F","weight":1,"opacity":0.5,"fill":false,"fillColor":"#03F","fillOpacity":0.2,"dashArray":null,"smoothFactor":1,"noClip":false},null,null,null,null,null]},{"method":"addPolylines","args":[[[[{"lng":[-93.636947,-93.635824],"lat":[42.05848,42.058194]}]]],null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"#03F","weight":1,"opacity":0.5,"fill":false,"fillColor":"#03F","fillOpacity":0.2,"dashArray":null,"smoothFactor":1,"noClip":false},null,null,null,null,null]},{"method":"addPolylines","args":[[[[{"lng":[-93.636947,-93.636047],"lat":[42.05848,42.057854]}]]],null,null,{"lineCap":null,"lineJoin":null,"clickable":true,"pointerEvents":null,"className":"","stroke":true,"color":"#03F","weight":1,"opacity":0.5,"fill":false,"fillColor":"#03F","fillOpacity":0.2,"dashArray":null,"smoothFactor":1,"noClip":false},null,null,null,null,null]}],"limits":{"lat":[41.98651,42.063304],"lng":[-93.692606,-93.596569]}},"evals":[],"jsHooks":[]}</script>


---

# _K_-Nearest Neighbors Model

Consider the 2-nearest neighbor model. Would there be a difference in the estimated model performance between re-prediction and cross-validation? 

`caret` has a `knnreg` function that can be used (the `kknn` package is another good option). It has a formula method and we'll use this to illustrate the model:


```r
library(caret)

knn_train_mod &lt;- knnreg(log10(Sale_Price) ~ Longitude + Latitude, 
                        data = ames_train,
                        k = 2)
knn_pred &lt;- predict(knn_train_mod, newdata = ames_train[, c("Longitude", "Latitude")])

cor(knn_pred, log10(ames_train$Sale_Price))^2
```

```
## [1] 0.8611535
```

```r
sqrt(mean((knn_pred - log10(ames_train$Sale_Price))^2))
```

```
## [1] 0.06612691
```


---
# Resampling a 2-Nearest Neighbor Model

That's pretty good but are we tricking ourselves? One of those two neighbors is always itself...

To resample, let's create another function to fit this model and follow the same resampling process as before:



```r
knn_fit &lt;- function(data_split, ...) 
  knnreg(..., data = analysis(data_split))

cv_splits$knn_mod &lt;- map(cv_splits$splits, knn_fit, formula = form, k = 2)

knn_res &lt;- map2_df(cv_splits$splits, cv_splits$knn_mod, model_perf) %&gt;% 
  rename(RMSE_knn = RMSE, R2_knn = R2)

## Merge in results:
cv_splits &lt;- cv_splits %&gt;% bind_cols(knn_res)

colMeans(knn_res)
```

```
##  RMSE_knn    R2_knn 
## 0.1119360 0.6179389
```


---
# Optimism and Correlation

The model appears to be a drastic improvement over simple linear regression but we are definitely getting highly optimistic results by re-predicting the training set. 

We can try to make a more formal assessment of the two current models. 

Both models used the _same_ resamples, so we have 10 estimates of performance that are matched.

Does the matching mean anything? 

Most likely **yes**. It is very common to see that there is a resample effect. Similar to repeated measures designs, we can expect a relationship between models and resamples. For example, some resamples will have the worst performance over different models and so on. 

In other words, there is usually a within-resample correlation. 


---

# The Resample Effect

&lt;img src="Part_2_Basic_Principals_files/figure-html/cv-corr-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Model Comparison Accounting for Within-Resample Correlation

With only two models, a paired _t_-test can be used to estimate the difference in RMSE between the models: 


```r
t.test(cv_splits$RMSE_simple, cv_splits$RMSE_knn, paired = TRUE)
```

```
## 
## 	Paired t-test
## 
## data:  cv_splits$RMSE_simple and cv_splits$RMSE_knn
## t = 12.375, df = 9, p-value = 5.924e-07
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.04128422 0.05975498
## sample estimates:
## mean of the differences 
##               0.0505196
```


Hothorn _et al_ (2012) is the [original paper](https://scholar.google.com/scholar?hl=en&amp;q=analysis+of+benchmark+experiments&amp;btnG=&amp;as_sdt=1%2C7&amp;as_sdtp=) on comparing models using resampling. 


---

# Overfitting

Overfitting occurs when a model inappropriately picks up on trends in the training set that do not generalize to new samples. 

When this occurs, assessments of the model based on the training set can show good performance that does not reproduce in future samples.   
  
Some models have specific ``knobs'' to control over-fitting

* neighborhood size in nearest neighbor models is an example

* the number if splits in a tree model

Often, poor choices for these parameters can result in overfitting

For example, the next slide shows a data set with two predictors. We want to be able to produce a line (i.e. decision boundary) that differentiates two classes of data.


---

# Two Class Example



.pull-left[
&lt;img src="Part_2_Basic_Principals_files/figure-html/two-class-2panel-1.png" width="90%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
On the next slide, two classification boundaries are shown for the a different model type not yet discussed.

The difference in the two panels is solely due to different choices in tuning parameters. 

One overfits the training data.
]

---

# Two Model Fits

&lt;img src="Part_2_Basic_Principals_files/figure-html/two-class-overfit-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

# Grid Search to Tune Models

We usually don't have two-dimensional data so a quantitative method for under measuring overfitting is needed. 

_Resampling_ fits that description. 

A simple method for tuning a model is to used _grid search_ where: 

1. A pre-defined candidate set of tuning parameter values is determined. 

1. Each parameter combination is resampled and performance is measured. 

1. A _profile_ of performance versus the tuning parameters can be used to choose the best parameter values. 

1. A **final** model is created that uses the optimal parameters and the entire training set. 

The bad news is that all of the models (except the final model) are discarded. 

The good news is that all of the models (except the final model) can be run in parallel. 

Let's look at the Ames _K_-NN model and evaluate _K_=1, 2, ..., 20 using the same 10-fold cross-validation as before. 



---

# The Performance Profile

&lt;img src="Part_2_Basic_Principals_files/figure-html/ames-knn-1.png" width="60%" style="display: block; margin: auto;" /&gt;

Each point is the average of the 10 resampling results. The best value is _K_=5. 


---

# The Performance Profile and the Individual Resamples

&lt;img src="Part_2_Basic_Principals_files/figure-html/ames-knn-rs-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Preprocessing, Feature Engineering, and Recipes
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
